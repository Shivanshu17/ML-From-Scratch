{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning linear regression is an algorithm that fits a line in the given data-set and hence helps determine the relationship, predict values of, the dependent variable Y using the independent variable (explanotary variable) X.\n",
    "We are going to implement linear regression using, the iterative optimization technique of finding the parameters, vanilla gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Feature set is \n",
      " [[ 0.97852419  0.92455041  0.85851857]\n",
      " [ 0.51976042  0.32056401  0.80391003]\n",
      " [ 0.9317216   0.15057869  0.86471609]\n",
      " [ 0.78869088  0.75622387  0.74278469]\n",
      " [ 0.68917156  0.32414605  0.23585787]\n",
      " [ 0.33046011  0.5238525   0.49533811]\n",
      " [ 0.86759617  0.40921653  0.32566359]\n",
      " [ 0.31287049  0.3995969   0.8253191 ]\n",
      " [ 0.28780861  0.68196634  0.83703102]\n",
      " [ 0.95015994  0.11576446  0.53697296]]\n",
      "The set of output values of test case is \n",
      " [[ 0.70239582]\n",
      " [ 0.75733771]\n",
      " [ 0.9964127 ]\n",
      " [ 0.63264805]\n",
      " [ 0.45813932]\n",
      " [ 0.15012015]\n",
      " [ 0.61330861]\n",
      " [ 0.0486289 ]\n",
      " [ 0.37916888]\n",
      " [ 0.10824838]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import random\n",
    "import math\n",
    "\n",
    "# creating a n-dimensional dataset. Here we are taking the no. of features to be 9\n",
    "# let m be the no. of training examples, we are taking the m to be 150\n",
    "X = np.random.rand(10,3)\n",
    "y = np.random.rand(10,1)\n",
    "print ('The Feature set is \\n',X)\n",
    "print ('The set of output values of test case is \\n', y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis is a function that we fit our dataset to. We first assume a hypothesis function with parameters - B0, B1, B2,..., Bn as equal to B0x0 + B1x1 + B2x2 +... + Bnxn,  where n is equal to the no. of features in the given data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_value(B,X) :\n",
    "    h=0\n",
    "    for i in range(3):\n",
    "        h+= B[i]*X[i]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm. Gradient descent itself is of many types - like stochastic gradient descent, batch gradient descent,etc. Here we will be using the vanilla batch gradient descent algorithm for our task of finding the lowest point of difference in our objective function (cost function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(X,y,B,i):\n",
    "    G = 0\n",
    "    for j in range(10):\n",
    "        x = X[j]\n",
    "        # after slicing the row of feature set that we want to evaluate our hypothesis function on, we call the hypothesis_value funtion to know its value.\n",
    "        h = hypothesis_value(B,x)\n",
    "        xi = X[:,i]\n",
    "        G = G + (h - y[j])*xi[j]\n",
    "    return G\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function determines whether our gradient descent algorithm has converged. We will be using the value of two last gradient runs of parameter updation step to observe whether the optimization algorithm has converged. Gradient descent has been proved to be a reliable indicator of whether the cost function has attained a local minima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convergence(g1,g2):\n",
    "    if abs(g1-g2) < 0.000001:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameter array is equal to \n",
      "\n",
      "[ 0.48784393 -0.01542429  0.27486055]\n"
     ]
    }
   ],
   "source": [
    "B = np.zeros(3)\n",
    "conv = True\n",
    "c= 0\n",
    "alpha = 0.5\n",
    "while conv:\n",
    "    gamma1 = 0\n",
    "    for i in range(3):\n",
    "        gamma = grad_descent(X,y,B,i)\n",
    "        B[i] = B[i] - alpha*gamma\n",
    "        gamma1 += gamma\n",
    "    if c==0:\n",
    "        gamma2 = gamma1-1\n",
    "    conv = convergence(gamma1,gamma2)\n",
    "    c+=1\n",
    "    gamma2 = gamma1\n",
    "    \n",
    "\n",
    "print ('The parameter array is equal to \\n')\n",
    "print (B)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will randomy take an array of n elements (representing a test case) and then using that as test set we will try to predict the value of the output variable (dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the test case of \n",
      " [ 0.35110989  0.0405294   0.84553279]\n",
      "The predicted value of the output variable is    0.403065300306\n"
     ]
    }
   ],
   "source": [
    "X_test = np.random.rand(3)\n",
    "print ('With the test case of \\n',X_test)\n",
    "h = hypothesis_value(B,X_test)\n",
    "print ('The predicted value of the output variable is   ', h)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
